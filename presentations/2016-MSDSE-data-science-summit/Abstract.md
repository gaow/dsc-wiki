Title: Dynamic statistical comparisons made simple

Dynamic statistical comparisons (DSCs) are an attempt to change the way that researchers perform statistical comparisons of methods. When a new statistical method is developed, it is almost inevitable that it will be useful to compare it to other methods for tackling the same problem. However, the way these comparisons are usually done is suboptimal in many ways. First, comparisons are usually performed by the research group that developed one of the methods, which almost inevitably favors that method. Furthermore, performing these kinds comparisons is incredibly time-consuming, requiring careful familiarization with software implementing the methods, and the creation of pipelines and scripts for running and comparing them. And in fast-moving fields such as in genomics, new methods or software updates appear so frequently that comparisons are out of date before they even appear. In summary, the current system results in a large amount of wasted effort, with multiple groups performing redundant and sub-optimal comparisons.

We have proposed a DSC system to aid in efficient statistical comparisons. The system ensures reproduciblity, and facilitates sharing, adaptation and extension of existing statistical comparisons for new method development projects. Here we present an implementation of the system under which researchers can easily compose many complex statistical workflows by mixing-matching individual computational routines in R, Python or command programs at will, and obtain organized benchmark report for numerous workflows and parameters evaluated.
