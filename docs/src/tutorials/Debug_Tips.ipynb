{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging in DSC2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate scripts\n",
    "Under the hood, DSC2 generates R, Python or Shell scripts based on computational routines provided in `exec` entries and executes these scripts to complete the computational tasks. There are often roughly two types of errors that may occur:\n",
    "\n",
    "* There is an error from user provided computational routine\n",
    "* There is an inconsistency between DSC script interface and user provided computational routine.\n",
    "\n",
    "In case of errors DSC will fail. However the automatically generated script will be saved to disk and DSC2 will point you to the script in question so that you can debug interactively. Here we look at an example from [a benchmark on application of Bayesian Linear Model in genomic prediction](https://github.com/stephenslab/dsc2-blm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "kernel": "sos",
    "output_cache": "[{\"output_type\":\"stream\",\"text\":\"INFO: DSC script exported to \\u001b[32mblm.html\\u001b[0m\\nINFO: Constructing DSC from \\u001b[32mblm.dsc\\u001b[0m ...\\nINFO: Building execution graph ...\\n\\rDSC:   0%|          | 0/10 [00:00<?, ?it/s]\\rDSC:  10%|█         | 1/10 [00:00<00:00,  9.67it/s]\\rDSC:  20%|██        | 2/10 [00:00<00:00,  9.12it/s]\\rDSC:  60%|██████    | 6/10 [00:00<00:00, 11.49it/s]\\rDSC:  90%|█████████ | 9/10 [00:00<00:00, 10.97it/s]\\n\\u001b[91mERROR\\u001b[0m: \\u001b[91mFailed to execute workflow DSC\\n[score_1 ['dsc_blm/datamaker.R_1_gemma_bs] RuntimeError:\\n\\tFailed to process statement sos_run('core_score_1:1', outp...ignature)\\\\n: Failed to execute workflow core_score_1\\n[core_score_1_1 (score.R)] RuntimeError:\\n\\tFailed to execute script (ret=1).\\nPlease use command\\n\\t\\u001b[32mRscript \\\\\\n\\t  --default-packages=datasets,methods,utils,stats,grDevices,graphics \\\\\\n\\t  /home/gaow/GIT/lab-dsc/dsc2-blm/.sos/core_score_1_1_6.R\\u001b[91m\\nunder \\\"/home/gaow/GIT/lab-dsc/dsc2-blm\\\" to test it.\\n[core_score_1] RuntimeError:\\n\\t1 failed step: core_score_1_1 (score.R)\\n[DSC] RuntimeError:\\n\\t1 failed step: score_1\\u001b[0m\\n\\u001b[95mWARNING\\u001b[0m: \\u001b[95mIf needed, you can open \\u001b[32mdsc_blm.transcript.html\\u001b[95m and use \\u001b[32mctrl-F\\u001b[95m to search for the problematic chunk of code by output file name.\\u001b[0m\\n\",\"name\":\"stderr\"}]"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: DSC script exported to \u001b[32mblm.html\u001b[0m\n",
      "INFO: Constructing DSC from \u001b[32mblm.dsc\u001b[0m ...\n",
      "INFO: Building execution graph ...\n",
      "\r",
      "DSC:   0%|          | 0/10 [00:00<?, ?it/s]\r",
      "DSC:  10%|█         | 1/10 [00:00<00:00,  9.67it/s]\r",
      "DSC:  20%|██        | 2/10 [00:00<00:00,  9.12it/s]\r",
      "DSC:  60%|██████    | 6/10 [00:00<00:00, 11.49it/s]\r",
      "DSC:  90%|█████████ | 9/10 [00:00<00:00, 10.97it/s]\n",
      "\u001b[91mERROR\u001b[0m: \u001b[91mFailed to execute workflow DSC\n",
      "[score_1 ['dsc_blm/datamaker.R_1_gemma_bs] RuntimeError:\n",
      "\tFailed to process statement sos_run('core_score_1:1', outp...ignature)\\n: Failed to execute workflow core_score_1\n",
      "[core_score_1_1 (score.R)] RuntimeError:\n",
      "\tFailed to execute script (ret=1).\n",
      "Please use command\n",
      "\t\u001b[32mRscript \\\n",
      "\t  --default-packages=datasets,methods,utils,stats,grDevices,graphics \\\n",
      "\t  /home/gaow/GIT/lab-dsc/dsc2-blm/.sos/core_score_1_1_6.R\u001b[91m\n",
      "under \"/home/gaow/GIT/lab-dsc/dsc2-blm\" to test it.\n",
      "[core_score_1] RuntimeError:\n",
      "\t1 failed step: core_score_1_1 (score.R)\n",
      "[DSC] RuntimeError:\n",
      "\t1 failed step: score_1\u001b[0m\n",
      "\u001b[95mWARNING\u001b[0m: \u001b[95mIf needed, you can open \u001b[32mdsc_blm.transcript.html\u001b[95m and use \u001b[32mctrl-F\u001b[95m to search for the problematic chunk of code by output file name.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! dsc -x blm.dsc -j8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "The error message is a bit verbose but the part most relevant to debugging are hightlighted in green color on your terminal. Towards the end of the error message you should see that the problematic script is saved to `.sos/core_score_1_1_6.R`. You can also see that a file called `dsc_blm.transcript.html` is generated. \n",
    "\n",
    "## Debug with the problematic script\n",
    "As prompted in the error message, let's try to execute `.sos/core_score_1_1_6.R`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "kernel": "ir",
    "output_cache": "[{\"output_type\":\"stream\",\"text\":\"Warning message in true.value - prediction:\\n“longer object length is not a multiple of shorter object length”Warning message in true.value - prediction:\\n“longer object length is not a multiple of shorter object length”\",\"name\":\"stderr\"},{\"output_type\":\"error\",\"ename\":\"ERROR\",\"evalue\":\"Error in cor(prediction, true.value): incompatible dimensions\\n\",\"traceback\":[\"Error in cor(prediction, true.value): incompatible dimensions\\nTraceback:\\n\",\"1. score(meta, prediction)\",\"2. cor(prediction, true.value)   # at line 13 of file <text>\"]}]"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in true.value - prediction:\n",
      "“longer object length is not a multiple of shorter object length”Warning message in true.value - prediction:\n",
      "“longer object length is not a multiple of shorter object length”"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in cor(prediction, true.value): incompatible dimensions\n",
     "output_type": "error",
     "traceback": [
      "Error in cor(prediction, true.value): incompatible dimensions\nTraceback:\n",
      "1. score(meta, prediction)",
      "2. cor(prediction, true.value)   # at line 13 of file <text>"
     ]
    }
   ],
   "source": [
    "## r script UUID: dcfc55ef9ae7c9eab6397eefe425afc0\n",
    "## BEGIN code by DSC2\n",
    "DSC_LIBPATH <- NULL\n",
    "source(\"/home/gaow/GIT/lab-dsc/dsc2-blm/.sos/.dsc/utils.R\")\n",
    "DSC_714A6C66AA <- list()\n",
    "input.files <- c('dsc_blm/datamaker.R_7.rds', 'dsc_blm/datamaker.R_7_gemma_bslmm_1.rds')\n",
    "for (i in 1:length(input.files)) DSC_714A6C66AA <- DSC_LMERGE(DSC_714A6C66AA, readRDS(input.files[i]))\n",
    "meta <- DSC_714A6C66AA$meta\n",
    "prediction <- DSC_714A6C66AA$prediction\n",
    "## END code by DSC2\n",
    "score = function(meta, prediction){\n",
    "  true.value = meta$true.value\n",
    "  mse = mean((true.value-prediction)^2)\n",
    "  rmse = sqrt(mean((true.value-prediction)^2))\n",
    "  pcor = cor(prediction, true.value)\n",
    "  slope = lm(true.value~prediction)$coef[2]\n",
    "  return(list(mse=mse, rmse=rmse, pcor=pcor, slope=slope))\n",
    "}\n",
    "output = score(meta, prediction)\n",
    "## BEGIN code by DSC2\n",
    "saveRDS(list(mse=output$mse, pcor=output$pcor, slope=output$slope), 'dsc_blm/datamaker.R_7_gemma_bslmm_1_score.R_1.rds')\n",
    "## END code by DSC2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "You see that the script was propagated by DSC2 with variable assignment and input / output specifications. Also this script failed because of `incompatible dimensions` as indidcated by R. So what's going on? The function call takes two variables, `meta` and `prediction`. Let's examine what these variables look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "kernel": "ir",
    "output_cache": "[{\"output_type\":\"stream\",\"text\":\"$true.value\\n       3881        3889       39359       41471       41484       41686 \\n 0.99831761  2.33609688  1.49504952  0.74828619  0.28221675  0.40967975 \\n      41800       41804       42274       62633       64869       78690 \\n 0.68604098 -0.45643508  0.44940658  0.38118361  0.52587135 -0.72295826 \\n      82710       83123       85599       85676       85711       85741 \\n-0.05990794  1.29932283  0.52996782  1.25576829  0.51239290  0.62052186 \\n      85786       96619      107059      133128      143058      143808 \\n 1.44805003  0.70355076  1.25938426  0.32484286  0.63805606  0.03753599 \\n     143989      217562      217777      218794      220384      292703 \\n 1.07711159 -0.48228439  0.04524029  0.15653729 -0.76760412  0.64203852 \\n     295146      295191      320849      321614      321822      321833 \\n 1.00396960  0.10798228 -0.68283238 -0.15248985  0.86163551  1.02641469 \\n     321840      323172      342294      343215      353782      359354 \\n 1.09444220  1.15157292  0.68958366  0.47540248 -0.46720970  0.59572313 \\n     369670      376770      377276      390604      393017      412956 \\n 0.10580781  0.55220931  0.13729283  0.85225353 -0.57501290  0.40696777 \\n     413235      419152      421018      421340      422181      427650 \\n-0.45474926 -0.16824864  0.56170530  1.00852215  1.75413716 -0.15158585 \\n     428988      430174      430433      454059      471420      609307 \\n 0.09478073  0.17486147  0.21150984  1.54804306 -0.76649652  1.19920765 \\n     653215      673831      675238      835884      877657      904516 \\n 1.16681050  0.29330898  0.41849979  0.03905079  1.26586695  0.57880787 \\n     934019     1210458     1290414     1298205     1410845     1437563 \\n 0.04472721 -1.53023212 -0.33237630 -0.95163591 -1.38193655 -0.55183484 \\n    1477928     1491661     1507236     1541043     1630947     1630957 \\n-0.89434230 -0.26762272 -1.28624360  0.35441402  0.13366871  0.15286431 \\n    1642433     1673085     1799716     1987795     2151597     2485043 \\n-0.12495471 -1.12012879 -0.93171548  0.07713251  0.57741524 -1.35715411 \\n    2586622     2588632     2621094     3826178     3827768     3827770 \\n-0.10364165 -0.46663147 -0.69299619 -1.22189722 -2.28909755 -2.31129831 \\n    3827774     3830167     3830201     3830206     3848815     3863169 \\n-2.05032125 -1.78563863 -1.69475069 -1.74084618 -1.70271560 -2.14879133 \\n    4057141     4057155     4059217     4059227 \\n-0.84147093 -1.02510366 -1.14887739 -0.49037147 \\n\\n[1] \\\"length meta:  1\\\"\\n [1]  0.1975210  0.2256670  0.5637760 -0.3171690 -0.6541230 -1.1402300\\n [7]  0.1176520 -0.3587820  0.1419840  0.2371910 -0.2146280 -0.6184410\\n[13]  1.1502500 -0.1172700  0.1651820  0.0489489 -0.2423130 -0.5081880\\n[19]  0.1954180  0.4832560 -0.1633740  0.2889630 -0.5772470 -0.9211860\\n[25] -0.2271150 -0.3913900  0.1802350  0.2908560  0.0703415  0.1579900\\n[31]  0.2151020 -0.0525270\\n[1] \\\"length prediction:  32\\\"\\n\",\"name\":\"stdout\"}]"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$true.value\n",
      "       3881        3889       39359       41471       41484       41686 \n",
      " 0.99831761  2.33609688  1.49504952  0.74828619  0.28221675  0.40967975 \n",
      "      41800       41804       42274       62633       64869       78690 \n",
      " 0.68604098 -0.45643508  0.44940658  0.38118361  0.52587135 -0.72295826 \n",
      "      82710       83123       85599       85676       85711       85741 \n",
      "-0.05990794  1.29932283  0.52996782  1.25576829  0.51239290  0.62052186 \n",
      "      85786       96619      107059      133128      143058      143808 \n",
      " 1.44805003  0.70355076  1.25938426  0.32484286  0.63805606  0.03753599 \n",
      "     143989      217562      217777      218794      220384      292703 \n",
      " 1.07711159 -0.48228439  0.04524029  0.15653729 -0.76760412  0.64203852 \n",
      "     295146      295191      320849      321614      321822      321833 \n",
      " 1.00396960  0.10798228 -0.68283238 -0.15248985  0.86163551  1.02641469 \n",
      "     321840      323172      342294      343215      353782      359354 \n",
      " 1.09444220  1.15157292  0.68958366  0.47540248 -0.46720970  0.59572313 \n",
      "     369670      376770      377276      390604      393017      412956 \n",
      " 0.10580781  0.55220931  0.13729283  0.85225353 -0.57501290  0.40696777 \n",
      "     413235      419152      421018      421340      422181      427650 \n",
      "-0.45474926 -0.16824864  0.56170530  1.00852215  1.75413716 -0.15158585 \n",
      "     428988      430174      430433      454059      471420      609307 \n",
      " 0.09478073  0.17486147  0.21150984  1.54804306 -0.76649652  1.19920765 \n",
      "     653215      673831      675238      835884      877657      904516 \n",
      " 1.16681050  0.29330898  0.41849979  0.03905079  1.26586695  0.57880787 \n",
      "     934019     1210458     1290414     1298205     1410845     1437563 \n",
      " 0.04472721 -1.53023212 -0.33237630 -0.95163591 -1.38193655 -0.55183484 \n",
      "    1477928     1491661     1507236     1541043     1630947     1630957 \n",
      "-0.89434230 -0.26762272 -1.28624360  0.35441402  0.13366871  0.15286431 \n",
      "    1642433     1673085     1799716     1987795     2151597     2485043 \n",
      "-0.12495471 -1.12012879 -0.93171548  0.07713251  0.57741524 -1.35715411 \n",
      "    2586622     2588632     2621094     3826178     3827768     3827770 \n",
      "-0.10364165 -0.46663147 -0.69299619 -1.22189722 -2.28909755 -2.31129831 \n",
      "    3827774     3830167     3830201     3830206     3848815     3863169 \n",
      "-2.05032125 -1.78563863 -1.69475069 -1.74084618 -1.70271560 -2.14879133 \n",
      "    4057141     4057155     4059217     4059227 \n",
      "-0.84147093 -1.02510366 -1.14887739 -0.49037147 \n",
      "\n",
      "[1] \"length meta:  1\"\n",
      " [1]  0.1975210  0.2256670  0.5637760 -0.3171690 -0.6541230 -1.1402300\n",
      " [7]  0.1176520 -0.3587820  0.1419840  0.2371910 -0.2146280 -0.6184410\n",
      "[13]  1.1502500 -0.1172700  0.1651820  0.0489489 -0.2423130 -0.5081880\n",
      "[19]  0.1954180  0.4832560 -0.1633740  0.2889630 -0.5772470 -0.9211860\n",
      "[25] -0.2271150 -0.3913900  0.1802350  0.2908560  0.0703415  0.1579900\n",
      "[31]  0.2151020 -0.0525270\n",
      "[1] \"length prediction:  32\"\n"
     ]
    }
   ],
   "source": [
    "source(\"/home/gaow/GIT/lab-dsc/dsc2-blm/.sos/.dsc/utils.R\")\n",
    "DSC_714A6C66AA <- list()\n",
    "input.files <- c('dsc_blm/datamaker.R_7.rds', 'dsc_blm/datamaker.R_7_gemma_bslmm_1.rds')\n",
    "for (i in 1:length(input.files)) DSC_714A6C66AA <- DSC_LMERGE(DSC_714A6C66AA, readRDS(input.files[i]))\n",
    "meta <- DSC_714A6C66AA$meta\n",
    "prediction <- DSC_714A6C66AA$prediction\n",
    "print(meta)\n",
    "print(paste(\"length meta: \", length(meta)))\n",
    "print(prediction)\n",
    "print(paste(\"length prediction: \", length(prediction)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "It looks like `meta$true.value` has a lot more elements than `prediction`. Therefore the statement `mse = mean((true.value-prediction)^2)` must fail. We know from the [DSC script](https://github.com/stephenslab/dsc2-blm/blob/master/blm.dsc) that since we have set `test_size = 100` we do expect both `prediction` and `true.value` to be of length 100. So there must be something wrong with `prediction`, that somehow it gets truncated. This variable comes from `dsc_blm/datamaker.R_7_gemma_bslmm_1.rds` so there is nothing we can do in this script to fix the problem. \n",
    "\n",
    "However in practice, perhaps you run into error in your DSC benchmark that can obviously be identified and fixed in this script. You should fix it, test it out interactively, and if it works, you should apply your patch to the original scripts that DSC calls because fixing these propagated intermediate scripts is not enough. In this example the `score` function seems fine so there is nothing to do to the [`score.R` script](https://github.com/stephenslab/dsc2-blm/blob/master/score.R). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "## Debugging with the entire transcript profile\n",
    "Now we want to find out which command has generated this set of problematic `prediction` data. That is, we want to find the command that generated `dsc_blm/datamaker.R_7_gemma_bslmm_1.rds` from which `prediction` is loaded. However the problematic script above did not tell us where this data comes from. This leads us to examining the entire transcript of the DSC benchmark, written in `dsc_blm.transcript.html`.\n",
    "\n",
    "You can open up this file in your web browser, and *search* (`ctrl-F`) for the phrase `dsc_blm/datamaker.R_7_gemma_bslmm_1.rds`. This leads you to the following (this is a screenshot of a chunk of `dsc_blm.transcript.html`): \n",
    "\n",
    "![debug-1](../../img/debug-tips-1.png)\n",
    "\n",
    "Unsurprisingly, running the code by itself produces no error message -- this is expected because otherwise DSC would have failed at this stage. Usually one should start interacting with this new piece of code, fixing it until the output looks good, and apply the patch to the original benchmark code, and finally run DSC again.\n",
    "\n",
    "For this example, what exactly is the problem that led to truncated result? Well, the issue is not very apparent unless you are aware of the parallel proessing nature of DSC and familiar with the use of DSC `File()` operator. However this is a very common mistake one can make and this is why we want to show it here. Please take a look at line 7 of the R code:\n",
    "\n",
    "```r\n",
    "result <- 'gemma.out'\n",
    "```\n",
    "\n",
    "This is code automatically generated by DSC, based on your DSC script. In the DSC script the corresponding section is:\n",
    "\n",
    "```\n",
    "gemma_bslmm(bayeslasso):\n",
    "  exec: gemma.bslmm.wrapper.R\n",
    "  .alias: gemma_bslmm\n",
    "  params:\n",
    "    result: gemma.out\n",
    "    .alias: args = List(w = nIter, s = burnin, result = result)\n",
    "\n",
    "```\n",
    "\n",
    "However it turns out here `gemma.out` is used as temporary file prefix for intermediate results from `gemma`, a command line program. It is directly called from shell, see Line 27, 37 and 42 of the R code. In practice what happens is all parallel DSC instances all creates / writes on the **same file** `gemma.out`, creating a I/O race. Therefore the file would have been corrupted, leading to truncated file.\n",
    "\n",
    "To fix the problem one should use `File()` operator for such cases:\n",
    "\n",
    "```\n",
    "gemma_bslmm(bayeslasso):\n",
    "  exec: gemma.bslmm.wrapper.R\n",
    "  .alias: gemma_bslmm\n",
    "  params:\n",
    "    result: File()\n",
    "    .alias: args = List(w = nIter, s = burnin, result = result)\n",
    "```\n",
    "\n",
    "This is the 2nd type of error listed in the beginning of this document, ie, an error in communication between DSC and your script. After fixing the error you can run DSC again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "kernel": "sos",
    "output_cache": "[]"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: DSC script exported to \u001b[32mblm.html\u001b[0m\n",
      "INFO: Constructing DSC from \u001b[32mblm.dsc\u001b[0m ...\n",
      "INFO: Building execution graph ...\n",
      "\r",
      "DSC:   0%|          | 0/10 [00:00<?, ?it/s]\r",
      "DSC:  10%|█         | 1/10 [00:00<00:00,  9.66it/s]\r",
      "DSC:  20%|██        | 2/10 [00:00<00:00,  8.92it/s]\r",
      "DSC:  50%|█████     | 5/10 [00:00<00:00, 11.07it/s]\r",
      "Running gemma_bslmm: \r",
      "Running core_gemma_bslmm_1 (gemma_bslmm): \r",
      "Running gemma_bslmm (00:02:00): \r",
      "Running core_gemma_bslmm_1 (gemma_bslmm) (00:02:00): \n",
      "\n",
      "\r",
      "          \r",
      "DSC:  70%|███████   | 7/10 [00:19<00:00, 11.07it/s]\r",
      "DSC:  80%|████████  | 8/10 [02:16<00:27, 13.69s/it]\r",
      "DSC:  90%|█████████ | 9/10 [02:19<00:10, 10.31s/it]\r",
      "DSC: 100%|██████████| 10/10 [02:19<00:00,  7.25s/it]\n",
      "INFO: Building output database \u001b[32mdsc_blm.rds\u001b[0m ...\n",
      "INFO: DSC complete!\n",
      "INFO: Elapsed time \u001b[32m140.619\u001b[0m seconds.\n"
     ]
    }
   ],
   "source": [
    "! dsc -x blm.dsc -j8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "You see that all `gemma_bslmm` related codes are re-executed and the DSC benchmark completed without an issue.\n",
    "\n",
    "## Log files\n",
    "The run-time information are kept in greater detail in `*.log` file. For the example above you should see `dsc_blm.log` generated for the DSC run. You can look into the log file for more run-time information to help performing diagnostics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos.jupyter.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "celltoolbar": true,
   "kernels": [
    [
     "sos",
     "SoS",
     ""
    ],
    [
     "ir",
     "ir",
     ""
    ],
    [
     "python3",
     "Python3",
     "#EAFAF1"
    ]
   ],
   "panel": {
    "displayed": true,
    "height": 0,
    "style": "side"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
