{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running DSC on a remote computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "DSC uses one additional configuration file and two command options to run on a remote computer. Here a remote computer can be:\n",
    "\n",
    "1. A standalone desktop workstation without a job queue system\n",
    "2. A system with a job queue manager, such as PBS-based cluster\n",
    "\n",
    "Conventionally, to run jobs on a remote computer (or host, hereafter), users will have to log in to the host from their local computer (or local, hereafter), copy over required resources and install required software, before performing any computation. For cluster systems users will not only have to write job files to submit, but also have to actively monitoring active jobs and keep submitting new ones, because otherwise submitting them all will likely hit the system's limit of permitted jobs and result in failure. For a DSC benchmark that contains relatively arbitrary number of computational tasks with implicit input and output and different resource requirements, it is nearly impossible to manually configure cluster jobs properly.\n",
    "\n",
    "Using a DSC job template and command options, DSC allows users to:\n",
    "\n",
    "1. Use remote host without having to login.\n",
    "2. Easily configure resource requirement per module.\n",
    "3. Submit the entire benchmark without worrying about job limits.\n",
    "4. Automatically sync files to the remote computer even if file path convention is different between local and host (eg `/Users/<username>` on Mac vs `/home/<username>` on Linux)\n",
    "\n",
    "Under the hood, DSC will utilize local resource to:\n",
    "\n",
    "1. Analyze DSC and build-up input / output dependencies\n",
    "2. Submitting jobs to remote in such a way that only maximum allowed jobs are submitted; the unsubmitted jobs are stashed on a local computer, which will keep monitoring the remote computer and keep submitting jobs to it as previous jobs complete.\n",
    "\n",
    "As one can probably tell, the local still has to be active and perform some computations (mostly monitoring, using one CPU thread) while the behchmark is being executed. Therefore the local has to be kept active -- the computer has to be on and not in \"suspend\" mode. We realize this might be inconvenient for laptop users. Though in principle one can use a [`screen`](https://www.gnu.org/software/screen/) on cluster's headnode as local and compute node as remote to keep the monitor run on the background, we discourage doing so because it is typically not good practice to have long running computations on a headnode. Users can either use an interactive section as \"local\" to submit jobs, or just keep a local laptop / desktop up and running throughout the entire benchmarking process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "DSC uses [`SoS`](https://vatlab.github.io/sos-docs) library to execute jobs. In order for remote submission to work, one needs to install `sos` package to the remote host, and additionally `sos-pbs` package if the remote uses a PBS system.To install SoS to remote host,\n",
    "\n",
    "```\n",
    "pip install sos sos-pbs\n",
    "```\n",
    "\n",
    "If you run into troubles, you may find [DSC installation guide](../installation.html) a useful resource to resolve problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remote configuration\n",
    "\n",
    "DSC command option `--host` accepts a YAML configuration file that specifies a *template* for remote jobs. **We provide support to such configuration files on need-basis**, because we (the DSC developers) can only verify and ensure the it works on system that we have access to and use on regular basis. For example, here is a template for a system running PBS type of queue via Slurm Workload Manage:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "DSC:\n",
    "  midway2:\n",
    "    description: UChicago RCC cluster Midway 2\n",
    "    address: gaow@midway2.rcc.uchicago.edu\n",
    "    paths:\n",
    "      home: /home/gaow\n",
    "    queue_type: pbs\n",
    "    wait_for_tasks: false\n",
    "    status_check_interval: 60\n",
    "    max_running_jobs: 30\n",
    "    max_cores: 40\n",
    "    max_walltime: \"36:00:00\"\n",
    "    max_mem: 64G\n",
    "    job_template: |\n",
    "      #!/bin/bash\n",
    "      #SBATCH --time={walltime}\n",
    "      #{partition}\n",
    "      #{account}\n",
    "      #SBATCH --nodes=1\n",
    "      #SBATCH --ntasks-per-node={cores}\n",
    "      #SBATCH --mem-per-cpu={mem//10**9}G\n",
    "      #SBATCH --job-name={job_name}\n",
    "      #SBATCH --output={cur_dir}/.sos/{job_name}.out\n",
    "      #SBATCH --error={cur_dir}/.sos/{job_name}.err\n",
    "      cd {cur_dir}\n",
    "      module load R/3.4.3\n",
    "    partition: \"SBATCH --partition=broadwl\"\n",
    "    account: \"\"\n",
    "    submit_cmd: sbatch {job_file}\n",
    "    submit_cmd_output: \"Submitted batch job {job_id}\"\n",
    "    status_cmd: squeue --job {job_id}\n",
    "    kill_cmd: scancel {job_id}\n",
    "  stephenslab:\n",
    "    based_on: hosts.midway2\n",
    "    max_cores: 28\n",
    "    max_mem: 128G\n",
    "    max_walltime: \"10d\"\n",
    "    partition: \"SBATCH --partition=mstephens\"\n",
    "    account: \"SBATCH --account=pi-mstephens\"\n",
    "\n",
    "\n",
    "default:\n",
    "  queue: midway2\n",
    "  time_per_instance: 10m\n",
    "  instances_per_job: 2\n",
    "  n_cpu: 1\n",
    "  mem_per_cpu: 2G\n",
    "\n",
    "simulate:\n",
    "  instances_per_job: 20\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The section `DSC` is required to configure all systems. Its syntax mostly follows from [SoS remote task specification](https://vatlab.github.io/sos-docs/doc/documentation/Remote_Execution.html). Here `midway2` is a host provided by [The University of Chicago RCC group](https://rcc.uchicago.edu). Jobs are submitted to `partition=broadwl`. Typically it has 40 cores per node, allows for 30 concurrent jobs per user, and a maximum running time of 36hrs per job. These limitations have been reflected by the `max_*` values in the configuration. `stephenslab` is a special partition on `midway2` that allows for different configurations under the same system, thus it is derived from `midway2` via `based_on: hosts.midway2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The section `default` is also required. It provides default settings for all modules in the DSC. Available settings are:\n",
    "\n",
    "- `queue`: name of the queue on the remote host that the DSC uses\n",
    "- `time_per_instance`: maximum computation time for each module instance.\n",
    "- `instance_per_job`: how many module instances to submit as one remote jobs. This is useful consolidating numerous light-weight module instances into one jobs submission. \n",
    "- `n_cpu` and `mem_per_cpu` specify the CPU and memory requirement of a module instance.\n",
    "\n",
    "For example for 100 module instances of `simulate` that each generates some data in under a minute, one can specify `time_per_instance: 1m` and `instance_per_job: 200`. Then a single job containing 200 simulations will be submitted to the host with a total of 200 minutes computation time reserved.\n",
    "\n",
    "Typically, `DSC` and `default` section for host configuration do not have to be changed for different projects. Users can carefully configure them once, and reuse for various projects. For Stephens Lab users for example, one can take the example from above and replace `gaow` with their UChicago cnetID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run remote jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Command\n",
    "\n",
    "```\n",
    "dsc ... --host /path/to/config.yml\n",
    "```\n",
    "\n",
    "will load remote configuration in `/path/to/config.yml` and submit jobs. \n",
    "\n",
    "Additionally, \n",
    "\n",
    "```\n",
    "dsc ... --host /path/to/config.yml --to-host file1 dir1 file2\n",
    "```\n",
    "\n",
    "will sync specified files and folders to the remote, if the particular benchmark requires these files and folders to execute (eg, data resource, shell executables, or scripts in `DSC::lib_path`).\n",
    "\n",
    "Caution that to successfully use `--host` and `--to-host`, the command program [`rsync`](https://rsync.samba.org) have to be available from the local computer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "default_kernel": "SoS",
   "kernels": [],
   "panel": {
    "displayed": true,
    "height": 0,
    "style": "side"
   },
   "version": "0.9.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
